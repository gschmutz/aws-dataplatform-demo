# =======================================================================
# Platform Name            aws-demo-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  1.12.0-preview
# =======================================================================
version: '3.5'
networks:
  default:
    name: aws-demo-platform
services:
  #  ================================== ksqlDB ========================================== #
  ksqldb-server-1:
    image: confluentinc/ksqldb-server:0.15.0
    hostname: ksqldb-server-1
    container_name: ksqldb-server-1
    labels:
      com.platys.restapi.title: ksqlDB Server REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8088
    ports:
      - 8088:8088
      - 1095:1095
    depends_on:
      - schema-registry-1
    environment:
      KSQL_LOG4J_ROOT_LOGLEVEL: INFO
      KSQL_LOG4J_OPTS: -Dlog4j.configuration=file:/etc/ksqldb/log4j.properties
      KSQL_LOG4J_PROCESSING_LOG_BROKERLIST: pkc-lq8v7.eu-central-1.aws.confluent.cloud:9092
      KSQL_LOG4J_PROCESSING_LOG_TOPIC: demo_processing_log
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: demo_processing_log
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 3
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_APPLICATION_ID: kafka-demo
      KSQL_KSQL_SERVICE_ID: kafka-demo
      KSQL_HOST_NAME: ksqldb-server-1
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: pkc-lq8v7.eu-central-1.aws.confluent.cloud:9092
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: ''
      #KSQL_SECURITY_PROTOCOL: SASL_SSL
      #KSQL_SASL_MECHANISM: PLAIN
      #KSQL_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="None" password="None";
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: 3
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 3
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_KSQL_HIDDEN_TOPICS: ^_.*,default_ksql_processing_log
#      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
#      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_SUPPRESS_ENABLED: 'False'
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: '-1'
      KSQL_CONFIG_DIR: /etc/ksql
      KSQL_KSQL_EXTENSION_DIR: /etc/ksqldb/ext/
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksql:/etc/ksqldb/ext
    restart: unless-stopped
  ksqldb-server-2:
    image: confluentinc/ksqldb-server:0.15.0
    hostname: ksqldb-server-2
    container_name: ksqldb-server-2
    labels:
      com.platys.restapi.title: ksqlDB Server REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8089
    ports:
      - 8089:8088
      - 1096:1095
    depends_on:
      - schema-registry-1
    environment:
      KSQL_LOG4J_ROOT_LOGLEVEL: INFO
      KSQL_LOG4J_OPTS: -Dlog4j.configuration=file:/etc/ksqldb/log4j.properties
      KSQL_LOG4J_PROCESSING_LOG_BROKERLIST: pkc-lq8v7.eu-central-1.aws.confluent.cloud:9092
      KSQL_LOG4J_PROCESSING_LOG_TOPIC: demo_processing_log
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: demo_processing_log
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 3
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_APPLICATION_ID: kafka-demo
      KSQL_KSQL_SERVICE_ID: kafka-demo
      KSQL_HOST_NAME: ksqldb-server-2
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: pkc-lq8v7.eu-central-1.aws.confluent.cloud:9092
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: ''
      #KSQL_SECURITY_PROTOCOL: SASL_SSL
      #KSQL_SASL_MECHANISM: PLAIN
      #KSQL_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="None" password="None";
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: 3
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 3
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_KSQL_HIDDEN_TOPICS: ^_.*,default_ksql_processing_log
#      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
#      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_SUPPRESS_ENABLED: 'False'
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: '-1'
      KSQL_CONFIG_DIR: /etc/ksql
      KSQL_KSQL_EXTENSION_DIR: /etc/ksqldb/ext/
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksql:/etc/ksqldb/ext
    restart: unless-stopped
  # Access the cli by running:
  # > docker exec -it ksqldb-cli ksql http://ksqldb-server-1:8088
  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.15.0
    container_name: ksqldb-cli
    hostname: ksqldb-cli
    depends_on:
      - ksqldb-server-1
    volumes:
      - ./data-transfer:/data-transfer
    entrypoint: /bin/sh
    tty: true
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: trivadis/apache-spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://${PUBLIC_IP}:8080
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 4040-4044:4040-4044
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      INIT_DAEMON_STEP: setup_spark
      MASTER: spark://spark-master:7077
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-1:
    image: trivadis/apache-spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 28111:28111
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: trivadis/apache-spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - 28112:28112
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-history:
    image: trivadis/apache-spark-worker:3.0.1-hadoop3.2
    command: /spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    container_name: spark-history
    hostname: spark-history
    labels:
      com.platys.webui.title: Spark History Server
      com.platys.webui.url: http://${PUBLIC_IP}:28117
      com.platys.restapi.title: Spark History Server
      com.platys.restapi.url: http://${PUBLIC_IP}:28117/api/v1
    expose:
      - 18080
    ports:
      - 28117:18080
    environment:
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-thrift-server:
    image: trivadis/apache-spark-master:3.0.1-hadoop3.2
    container_name: spark-thrift-server
    ports:
      - 28118:10000
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
    command: bash -c "sleep 2m && /spark/sbin/start-thriftserver.sh && tail -f /spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-*.out"
  #  ================================== StreamSets DataCollector ========================================== #
  streamsets-1:
    image: streamsets/datacollector:3.18.1
    container_name: streamsets-1
    hostname: streamsets-1
    labels:
      com.platys.webui.title: StreamSets Data Collector UI
      com.platys.webui.url: http://${PUBLIC_IP}:18630
      com.platys.restapi.title: StreamSets Data Collector REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:18630/collector/restapi
    ports:
      - 18630:18630
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
      SDC_JAVA_OPTS: -Xmx2g -Xms2g
      SDC_JAVA8_OPTS: -XX:+UseG1GC
      SDC_CONF_MONITOR_MEMORY: 'true'
      SDC_CONF_PIPELINE_MAX_RUNNERS_COUNT: 50
      SDC_CONF_http_authentication: form
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/streamsets/pre-docker-entrypoint.sh:/pre-docker-entrypoint.sh
      - ./plugins/streamsets/user-libs:/opt/streamsets-datacollector-user-libs:Z
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    user: '1000'
    command:
      - dc
      - -exec
    entrypoint:
      - /pre-docker-entrypoint.sh
    restart: unless-stopped
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: trivadis/apache-zeppelin:0.9.0-spark3.0-hadoop3.2
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.webui.title: Apache Zeppelin UI
      com.platys.webui.url: http://${PUBLIC_IP}:28080
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      SPARK_HADOOP_FS_S3A_SECRET_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: s3.amazonaws.com
      AWS_DEFAULT_REGION: eu-central-1
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-avro_2.12:3.0.1,io.delta:delta-core_2.12:0.8.0
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_MEM: -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
      ZEPPELIN_ADMIN_USERNAME: admin
      ZEPPELIN_ADMIN_PASSWORD: changeme
      ZEPPELIN_USER_USERNAME: zeppelin
      ZEPPELIN_USER_PASSWORD: changeme
      # set spark-master for Zeppelin interpreter
      ZEPPELIN_SPARK_MASTER: spark://spark-master:7077
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: 'True'
      PYSPARK_PYTHON: python3
      SPARK_SUBMIT_OPTIONS: --conf spark.ui.port=4050 --conf spark.driver.host=${DOCKER_HOST_IP} --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/s3cfg:/root/.s3cfg.template
    restart: unless-stopped
  #  ================================== Redash ========================================== #
  redash-server:
    image: redash/redash:preview
    hostname: redash-server
    container_name: redash-server
    labels:
      com.platys.webui.title: Redash UI
      com.platys.webui.url: http://${PUBLIC_IP}:28161
    ports:
      - 28161:5000
    command: server
    environment:
      REDASH_REDIS_URL: redis://redash-redis:6379/0
      REDASH_DATABASE_URL: postgresql://postgres@redash-postgresql/postgres
      REDASH_PASSWORD_LOGIN_ENABLED: 'true'
      REDASH_RATELIMIT_ENABLED: 'false'
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  redash-scheduler:
    image: redash/redash:preview
    hostname: redash-scheduler
    container_name: redash-scheduler
    command: scheduler
    environment:
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: redis://redash-redis:6379/0
      REDASH_DATABASE_URL: postgresql://postgres@redash-postgresql/postgres
      REDASH_PASSWORD_LOGIN_ENABLED: 'true'
      REDASH_RATELIMIT_ENABLED: 'false'
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  redash-worker:
    image: redash/redash:preview
    hostname: redash-worker
    container_name: redash-worker
    command: worker
    environment:
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: redis://redash-redis:6379/0
      REDASH_DATABASE_URL: postgresql://postgres@redash-postgresql/postgres
      REDASH_PASSWORD_LOGIN_ENABLED: 'true'
      REDASH_RATELIMIT_ENABLED: 'false'
      PYTHONUNBUFFERED: 0
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  redash-postgresql:
    image: postgres:9.5-alpine
    hostname: redash-postgresql
    container_name: redash-postgresql
    # The following turns the DB into less durable, but gains significant performance improvements for the tests run (x3
    # improvement on my personal machine). We should consider moving this into a dedicated Docker Compose configuration for
    # tests.
    ports:
      - 15432:5432
    command: postgres -c fsync=off -c full_page_writes=off -c synchronous_commit=OFF
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  redash-db-setup:
    image: redash/redash:preview
    hostname: redash-db-setup
    container_name: redash-db-setup
    command: create_db
    depends_on:
      - redash-postgresql
    environment:
      REDASH_DATABASE_URL: postgresql://postgres@redash-postgresql/postgres
  redash-redis:
    image: redis:6.0.9
    hostname: redash-redis
    container_name: redash-redis
    ports:
      - 6385:6379
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== cAdvisor ========================================== #
  wetty:
    image: svenihoney/wetty:latest
    container_name: wetty
    hostname: wetty
    labels:
      com.platys.webui.title: WeTTY UI
      com.platys.webui.url: http://${PUBLIC_IP}:3001
    ports:
      - 3001:3000
    environment:
      - REMOTE_SSH_SERVER=${DOCKER_HOST_IP}
      - REMOTE_SSH_PORT=22
      - REMOTE_SSH_USER=
      - WETTY_PORT=3000
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: minimum/markdown-web:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://${PUBLIC_IP}:80
    ports:
      - 80:80
    volumes:
      - ./documentation:/home/python/markdown
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
    volumes:
      - ./documentation/templates:/templates
      - ./documentation/templates:/scripts
      - .:/variables
      - ./documentation:/output
      - ./data-transfer:/data-transfer
